{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Controllers for Kubernetes \u00b6 AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. This is a new open source project built with \u2764\ufe0f by AWS and available as a Developer Preview . We encourage you to try it out, provide feedback and contribute to development. Important: Because this project is a preview, there may be significant and breaking changes introduced in the future. We encourage you to try and experiment with this project. Please do not adopt it for production use. Background \u00b6 Kubernetes applications often require a number of supporting resources like databases, message queues, and object stores to operate. AWS provides a set of managed services that you can use to provide these resources for your applications, but provisioning and integrating them with Kubernetes was complex and time consuming. ACK lets you define and consume many AWS services and resources directly within a Kubernetes cluster. ACK gives you a unified, operationally seamless way to manage your application and its dependencies. How it works \u00b6 ACK is a collection of Kubernetes Custom Resource Definitions (CRDs) and controllers which work together to extend the Kubernetes API and create AWS resources on your cluster\u2019s behalf. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend. Getting started \u00b6 Until we've graduated ACK service controllers we ask you to test-drive them. Getting help \u00b6 For help, please consider the following venues (in order): Search open issues File an issue Join our mailing list . Chat with us on the #provider-aws channel in the Kubernetes Slack community.","title":"Overview"},{"location":"#aws-controllers-for-kubernetes","text":"AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. This is a new open source project built with \u2764\ufe0f by AWS and available as a Developer Preview . We encourage you to try it out, provide feedback and contribute to development. Important: Because this project is a preview, there may be significant and breaking changes introduced in the future. We encourage you to try and experiment with this project. Please do not adopt it for production use.","title":"AWS Controllers for Kubernetes"},{"location":"#background","text":"Kubernetes applications often require a number of supporting resources like databases, message queues, and object stores to operate. AWS provides a set of managed services that you can use to provide these resources for your applications, but provisioning and integrating them with Kubernetes was complex and time consuming. ACK lets you define and consume many AWS services and resources directly within a Kubernetes cluster. ACK gives you a unified, operationally seamless way to manage your application and its dependencies.","title":"Background"},{"location":"#how-it-works","text":"ACK is a collection of Kubernetes Custom Resource Definitions (CRDs) and controllers which work together to extend the Kubernetes API and create AWS resources on your cluster\u2019s behalf. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend.","title":"How it works"},{"location":"#getting-started","text":"Until we've graduated ACK service controllers we ask you to test-drive them.","title":"Getting started"},{"location":"#getting-help","text":"For help, please consider the following venues (in order): Search open issues File an issue Join our mailing list . Chat with us on the #provider-aws channel in the Kubernetes Slack community.","title":"Getting help"},{"location":"services/","text":"Supported services \u00b6 The following services are currently supported by ACK and for details see the Service Controller Release Roadmap : AWS Service Status Controller Amazon API Gateway V2 DEVELOPER PREVIEW apigatewayv2 Amazon DynamoDB BUILD dynamodb Amazon ECR DEVELOPER PREVIEW ecr Amazon S3 DEVELOPER PREVIEW s3 Amazon SQS BUILD sqs Amazon SNS DEVELOPER PREVIEW sns","title":"Services"},{"location":"services/#supported-services","text":"The following services are currently supported by ACK and for details see the Service Controller Release Roadmap : AWS Service Status Controller Amazon API Gateway V2 DEVELOPER PREVIEW apigatewayv2 Amazon DynamoDB BUILD dynamodb Amazon ECR DEVELOPER PREVIEW ecr Amazon S3 DEVELOPER PREVIEW s3 Amazon SQS BUILD sqs Amazon SNS DEVELOPER PREVIEW sns","title":"Supported services"},{"location":"community/background/","text":"Background \u00b6 In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize. Existing custom controllers \u00b6 AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book. Related projects \u00b6 Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Background"},{"location":"community/background/#background","text":"In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize.","title":"Background"},{"location":"community/background/#existing-custom-controllers","text":"AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book.","title":"Existing custom controllers"},{"location":"community/background/#related-projects","text":"Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Related projects"},{"location":"community/discussions/","text":"Discussions \u00b6 For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/discussions/#discussions","text":"For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Service Broker \u00b6 Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account. Cluster API \u00b6 Question Does the planned ACK service controller for EKS replace Kubernetes Cluster API ? Answer No, the ACK service controller for EKS does not replace Kubernetes Cluster API. Cluster API does a lot of really cool things and is designed to be a generic way to create Kubernetes clusters that run anywhere. It makes some different design decisions with that goal in mind. Some differences include: Cluster API is treated as your source of truth for all infrastructure. This means things like the cluster autoscaler need to be configured to use cluster api instead of AWS cloud provider. Generic Kubernetes clusters rely on running more services in the cluster and not services from AWS. Things like metrics and logging will likely need to run inside Kubernetes instead of using services like CloudWatch. IAM permission for Cluster-API Provider AWS (CAPA) need to be more broad than the ACK service controller for EKS because CAPA is responsible for provisioning everything needed for the cluster (VPC, gateway, etc). You don't need to run all of the ACK controllers if all you want is a way to provision an EKS cluster. You can pick and choose which ACK controllers you want to deploy. With the EKS ACK controller you will get all of the configuration flexibility of the EKS API including things like managed node groups and fargate. This is because the ACK service controller for EKS is built directly from the EKS API spec and not abstracted to be a general Kubernetes cluster. cdk8s \u00b6 Question How does ACK relate to cdk8s ? Answer cdk8s is an open-source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object-oriented APIs. You can use cdk8s to create any resource inside a Kubernetes cluster. This includes Custom Resources (CRs). All of the ACK controllers watch for specific CRs and you can generate those resources using cdk8s or any Kubernetes tooling. The two projects complement each other. cdk8s can create the Kubernetes resources and ACK uses those resources to create the AWS infrastructure. Contributing \u00b6 Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the contributor docs .","title":"FAQ"},{"location":"community/faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"community/faq/#service-broker","text":"Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account.","title":"Service Broker"},{"location":"community/faq/#cluster-api","text":"Question Does the planned ACK service controller for EKS replace Kubernetes Cluster API ? Answer No, the ACK service controller for EKS does not replace Kubernetes Cluster API. Cluster API does a lot of really cool things and is designed to be a generic way to create Kubernetes clusters that run anywhere. It makes some different design decisions with that goal in mind. Some differences include: Cluster API is treated as your source of truth for all infrastructure. This means things like the cluster autoscaler need to be configured to use cluster api instead of AWS cloud provider. Generic Kubernetes clusters rely on running more services in the cluster and not services from AWS. Things like metrics and logging will likely need to run inside Kubernetes instead of using services like CloudWatch. IAM permission for Cluster-API Provider AWS (CAPA) need to be more broad than the ACK service controller for EKS because CAPA is responsible for provisioning everything needed for the cluster (VPC, gateway, etc). You don't need to run all of the ACK controllers if all you want is a way to provision an EKS cluster. You can pick and choose which ACK controllers you want to deploy. With the EKS ACK controller you will get all of the configuration flexibility of the EKS API including things like managed node groups and fargate. This is because the ACK service controller for EKS is built directly from the EKS API spec and not abstracted to be a general Kubernetes cluster.","title":"Cluster API"},{"location":"community/faq/#cdk8s","text":"Question How does ACK relate to cdk8s ? Answer cdk8s is an open-source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object-oriented APIs. You can use cdk8s to create any resource inside a Kubernetes cluster. This includes Custom Resources (CRs). All of the ACK controllers watch for specific CRs and you can generate those resources using cdk8s or any Kubernetes tooling. The two projects complement each other. cdk8s can create the Kubernetes resources and ACK uses those resources to create the AWS infrastructure.","title":"cdk8s"},{"location":"community/faq/#contributing","text":"Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the contributor docs .","title":"Contributing"},{"location":"dev-docs/code-generation/","text":"Code generation \u00b6 In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services. Options considered \u00b6 To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ACK controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ACK is a collection of controllers that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community. Our approach \u00b6 We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. We will use the model files from the aws-sdk-go source repository as our source of truth and use the aws-sdk-go/private/model/api Go package to navigate that model. Note This step is the ack-generate apis command. After generating Kubernetes API type definitions for the top-level resources exposed by the AWS API, we then need to generate the \"DeepCopy\" interface implementations that enable those top-level resources and type definitions to be used by the Kubernetes runtime package (it defines an interface called runtime.Object that requires certain methods that copy the object and its component parts). Note This step runs the controller-gen object command Next, we generate the custom resource definition (CRD) configuration files, one for each top-level resource identified in earlier steps. Note This step runs the controller-gen crd command Next, we generate the actual implementation of the ACK controller for the target service. This step uses a set of templates and code in the pkg/model Go package to construct the service-specific resource management and linkage with the aws-sdk-go client for the service. Along with these controller implementation Go files, this step also outputs a set of Kubernetes configuration files for the Deployment and the ClusterRoleBinding of the Role created in the next step. Note This step runs the ack-generate controller command Finally, we generate the configuration file for a Kubernetes Role that the Kubernetes Pod (running in a Kubernetes Deployment ) running the ACK service controller. This Role needs to have permissions to read and write CRs of the Kind that the service controller manages. Note This step runs the controller-gen rbac command","title":"Code generation"},{"location":"dev-docs/code-generation/#code-generation","text":"In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services.","title":"Code generation"},{"location":"dev-docs/code-generation/#options-considered","text":"To generate custom resource (definitions) and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ACK controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ACK is a collection of controllers that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community.","title":"Options considered"},{"location":"dev-docs/code-generation/#our-approach","text":"We ended up with a hybrid custom+controller-runtime, using multiple phases of code generation: The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. We will use the model files from the aws-sdk-go source repository as our source of truth and use the aws-sdk-go/private/model/api Go package to navigate that model. Note This step is the ack-generate apis command. After generating Kubernetes API type definitions for the top-level resources exposed by the AWS API, we then need to generate the \"DeepCopy\" interface implementations that enable those top-level resources and type definitions to be used by the Kubernetes runtime package (it defines an interface called runtime.Object that requires certain methods that copy the object and its component parts). Note This step runs the controller-gen object command Next, we generate the custom resource definition (CRD) configuration files, one for each top-level resource identified in earlier steps. Note This step runs the controller-gen crd command Next, we generate the actual implementation of the ACK controller for the target service. This step uses a set of templates and code in the pkg/model Go package to construct the service-specific resource management and linkage with the aws-sdk-go client for the service. Along with these controller implementation Go files, this step also outputs a set of Kubernetes configuration files for the Deployment and the ClusterRoleBinding of the Role created in the next step. Note This step runs the ack-generate controller command Finally, we generate the configuration file for a Kubernetes Role that the Kubernetes Pod (running in a Kubernetes Deployment ) running the ACK service controller. This Role needs to have permissions to read and write CRs of the Kind that the service controller manages. Note This step runs the controller-gen rbac command","title":"Our approach"},{"location":"dev-docs/overview/","text":"Overview \u00b6 This section of the docs is for ACK contributors. The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts. In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions. Last but not least, in the testing section we show you how to test ACK locally.","title":"Overview"},{"location":"dev-docs/overview/#overview","text":"This section of the docs is for ACK contributors. The code generation section gives you a bit of background on how we go about automating the code generation for controllers and supporting artifacts. In the setup section we walk you through setting up your local Git environment with the repo and how advise you on how we handle contributions. Last but not least, in the testing section we show you how to test ACK locally.","title":"Overview"},{"location":"dev-docs/setup/","text":"Setup \u00b6 We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK). No matter if you're contributing code or docs, follow the steps below. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy. Fork the upstream repository \u00b6 First, fork the upstream source repository into your personal GitHub account. Then, in $GOPATH/src/github.com/aws/ , clone your repo and add the upstream like so: 1 2 3 git clone git@github.com:$GITHUB_ID/aws-controllers-k8s && \\ cd aws-controllers-k8s && \\ git remote add upstream git@github.com:aws/aws-controllers-k8s Go version We recommend to use a Go version of 1.14 or above for development. Create your local branch \u00b6 Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main Commit changes \u00b6 Make your changes locally, commit and push using: 1 2 3 git commit - a - m \"improves the docs a lot\" git push origin $ BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs Create a pull request \u00b6 Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day.","title":"Setup"},{"location":"dev-docs/setup/#setup","text":"We walk you now through the setup to start contributing to the AWS Controller for Kubernetes (ACK). No matter if you're contributing code or docs, follow the steps below. Issue before PR Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy.","title":"Setup"},{"location":"dev-docs/setup/#fork-the-upstream-repository","text":"First, fork the upstream source repository into your personal GitHub account. Then, in $GOPATH/src/github.com/aws/ , clone your repo and add the upstream like so: 1 2 3 git clone git@github.com:$GITHUB_ID/aws-controllers-k8s && \\ cd aws-controllers-k8s && \\ git remote add upstream git@github.com:aws/aws-controllers-k8s Go version We recommend to use a Go version of 1.14 or above for development.","title":"Fork the upstream repository"},{"location":"dev-docs/setup/#create-your-local-branch","text":"Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set BRANCH_NAME=docs-improve and then: 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/main","title":"Create your local branch"},{"location":"dev-docs/setup/#commit-changes","text":"Make your changes locally, commit and push using: 1 2 3 git commit - a - m \"improves the docs a lot\" git push origin $ BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs","title":"Commit changes"},{"location":"dev-docs/setup/#create-a-pull-request","text":"Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and there open the pull request as depicted below: We monitor the GitHub repo and try to follow up with comments within a working day.","title":"Create a pull request"},{"location":"dev-docs/testing/","text":"Testing \u00b6 Welcome to the ACK developer preview! In the following, we will take you through the steps to test ACK for the currently supported AWS services: Amazon ECR Amazon S3 Amazon SNS Amazon API Gateway V2 If you run into any problems when testing one of the above services, raise an issue with the details so we can reproduce your issue. Prerequisites \u00b6 For local development and testing we use \"Kubernetes in Docker\" ( kind ), which in turn requires Docker. Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can take up to 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally, and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time. In summary, in order to test ACK you will need to have the following tools installed and configured: Docker kind kubernetes-sigs/controller-tools AWS CLI version 1 jq make To build and test an ACK controller with kind , execute the commands as described in the following from the root directory of your checked-out source repository . Recommended RAM Given that our test setup creates the container images and then launches a test cluster, we recommend that you have at least 4GB of RAM available for the tests. With the prerequisites out of the way, let's move on to the first step: building the code generator. Build code generator \u00b6 To build the latest ack-generate binary, execute the following command: 1 make build-ack-generate One-off build You only have to do this once, overall. In other words: unless we change something upstream in terms of the code generation process, this is a one-off operation. Internally, the Makefile executes an go build here. Don't worry if you forget this step, the script in the next step will complain with a message along the line of ERROR: Unable to find an ack-generate binary and will give you another opportunity to rectify the situation. Build an ACK service controller \u00b6 Now that we have the basic code generation step done we will create the respective ACK service controller and its supporting artifacts. So first you have to select a service that you want to build and test. You do that by setting the SERVICE environment variable. Let's say we want to test the S3 service (creating an S3 bucket), so we would execute the following: 1 export SERVICE = s3 Now we are in a position to generate the ACK service controller for the S3 API. The following outputs the generated code to the services/$SERVICE directory: 1 make build-controller SERVICE=$SERVICE Handle controller-gen: command not found If you run into the controller-gen: command not found message when executing make build-controller then you want to check if the controller-gen binary is available in $GOPATH/bin , see also #234 . In addition to the ACK service controller code, above generates the custom resource definition (CRD) manifests as well as the necessary RBAC settings using the build-controller.sh script. Beyond dev preview In future, this step will also generate the end-user install artifacts, that is the Helm chart etc. that can be used to install those CRD manifests, and a deployment manifest that runs the ACK service controller in a pod. Now that we have the generation part completed, we want to see if the generated artifacts indeed are able to create an S3 bucket for us. You don't have to do the next steps, if all you want to test is if the generation works, however, for an end-to-end test, the next, final step is a necessary one. Run tests \u00b6 Time to run the end-to-end test. IAM setup \u00b6 In order for the ACK service controller to manage the S3 bucket, it needs an identity. In other words, it needs an IAM role that represents the ACK service controller towards the S3 service. First, define the name of the IAM role that will have the permission to manage S3 buckets on your behalf: 1 export ACK_TEST_IAM_ROLE = Admin - k8s Now we need to verify the IAM principal (likely an IAM user) that is going to assume the IAM role ACK_TEST_IAM_ROLE . So to get its ARN, execute: 1 export ACK_TEST_PRINCIPAL_ARN =$ ( aws sts get - caller - identity -- query 'Arn' -- output text ) You can verify if that worked using echo $ACK_TEST_PRINCIPAL_ARN and that should print something along the lines of arn:aws:iam::1234567890121:user/ausername . Next up, create the IAM role, adding the necessary trust relationship to the role, using the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 cat > trust-policy.json << EOF { \"Version\": \"2012-10-17\", \"Statement\": { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"$ACK_TEST_PRINCIPAL_ARN\" }, \"Action\": \"sts:AssumeRole\" } } EOF Using above trust policy, we can now create the IAM role: 1 2 3 aws iam create-role \\ --role-name $ACK_TEST_IAM_ROLE \\ --assume-role-policy-document file://trust-policy.json Now we're in the position to give the IAM role ACK_TEST_IAM_ROLE the permission to handle S3 buckets for us, using: 1 2 3 aws iam attach-role-policy \\ --role-name $ACK_TEST_IAM_ROLE \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonS3FullAccess\" Access delegation in IAM If you're not that familiar with IAM access delegation, we recommend you to peruse the IAM documentation Next, in order for our test to generate temporary credentials we need to tell it to use the IAM role we created in the previous step. To generate the IAM role ARN, do: 1 2 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query 'Account' -- output text ) && \\ export AWS_ROLE_ARN = arn : aws : iam :: $ { AWS_ACCOUNT_ID }: role /$ { ACK_TEST_IAM_ROLE } Info The tests uses the generate_temp_creds function from the scripts/lib/aws.sh script, executing effectively aws sts assume-role --role-session-arn $AWS_ROLE_ARN --role-session-name $TEMP_ROLE which fetches temporarily AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and an AWS_SESSION_TOKEN used in turn to authentication the ACK controller. The duration of the session token is 900 seconds (15 minutes). Phew that was a lot to set up, but good news: you're almost there. Run end-to-end test \u00b6 Before you proceed, make sure that you've done the IAM setup in the previous step. IAM troubles?! If you try the following command and you see an error message containing something along the line of AWS_ROLE_ARN is not defined. then you know that somewhere in the IAM setup you either left out a step or one of the commands failed. Now we're finally in the position to execute the end-to-end test: 1 make kind-test SERVICE=$SERVICE This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller is created, updated and deleted appropriately (still TODO). Finally, it will run tests that create resources for the respective service and verify if the resource has successfully created. In our example case it should create an S3 bucket and then destroy it again, yielding something like the following (edited down to the relevant parts): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ... ./ scripts / kind - build - test . sh - s s3 Using Kubernetes kindest / node : v1 . 16.9 @ sha256 : 7175872357 bc85847ec4b1aba46ed1d12fa054c83ac7a8a11f5c268957fd5765 Creating k8s cluster using \"kind\" ... No kind clusters found . Created k8s cluster using \"kind\" Building s3 docker image Building 's3' controller docker image with tag : ack - s3 - controller : ec452ed sha256 : c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef Loading the images into the cluster Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-worker\" , loading ... Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-control-plane\" , loading ... Loading CRD manifests for s3 into the cluster customresourcedefinition . apiextensions . k8s . io / buckets . s3 . services . k8s . aws created Loading RBAC manifests for s3 into the cluster clusterrole . rbac . authorization . k8s . io / ack - controller - role created clusterrolebinding . rbac . authorization . k8s . io / ack - controller - rolebinding created Loading service controller Deployment for s3 into the cluster 2020 / 08 / 18 09 : 51 : 46 Fixed the missing field by adding apiVersion : kustomize . config . k8s . io / v1beta1 Fixed the missing field by adding kind : Kustomization namespace / ack - system created deployment . apps / ack - s3 - controller created Running aws sts assume - role -- role - arn arn : aws : iam :: 1234567890121 : role / Admin - k8s , -- role - session - name tmp - role - 1 b779de5 -- duration - seconds 900 , Temporary credentials generated deployment . apps / ack - s3 - controller env updated Added AWS Credentials to env vars map ====================================================================================================== To poke around your test manually : export KUBECONFIG =/ Users / hausenbl / ACK / upstream / aws - controllers - k8s / scripts /../ build / tmp - test - ccc3c7f1 / kubeconfig kubectl get pods - A ====================================================================================================== bucket . s3 . services . k8s . aws / ack - test - smoke - s3 created { \"Name\" : \"ack-test-smoke-s3\" , \"CreationDate\" : \"2020-08-18T08:52:04+00:00\" } bucket . s3 . services . k8s . aws \"ack-test-smoke-s3\" deleted smoke took 27 second ( s ) \ud83e\udd51 Deleting k8s cluster using \"kind\" Deleting cluster \"test-ccc3c7f1\" ... As you can see, in above case the end-to-end test (creating cluster, deploying ACK, applying custom resources, and tear-down) took less than 30 seconds. This is for the warmed caches case. Background \u00b6 We use mockery for unit testing. You can install it by following the guideline on mockery's GitHub or simply by running our handy scirpt at ./scripts/install_mockery.sh for general Linux environments. We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there. Clean up \u00b6 To clean up a kind cluster, including the container images and configuration files created by the script specifically for said test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME If you want to delete all kind cluster running on your machine, use: 1 make delete-all-kind-clusters With this the testing is completed. Thanks for your time and we appreciate your feedback.","title":"Testing"},{"location":"dev-docs/testing/#testing","text":"Welcome to the ACK developer preview! In the following, we will take you through the steps to test ACK for the currently supported AWS services: Amazon ECR Amazon S3 Amazon SNS Amazon API Gateway V2 If you run into any problems when testing one of the above services, raise an issue with the details so we can reproduce your issue.","title":"Testing"},{"location":"dev-docs/testing/#prerequisites","text":"For local development and testing we use \"Kubernetes in Docker\" ( kind ), which in turn requires Docker. Footprint When you run the scripts/kind-build-test.sh script the first time, the step that builds the container image for the target ACK service controller can take up to 40 or more minutes. This is because the container image contains a lot of dependencies. Once you successfully build the target image this base image layer is cached locally, and the build takes a much shorter amount of time. We are aware of this (and the storage footprint, ca. 3 GB) and aim to reduce both in the fullness of time. In summary, in order to test ACK you will need to have the following tools installed and configured: Docker kind kubernetes-sigs/controller-tools AWS CLI version 1 jq make To build and test an ACK controller with kind , execute the commands as described in the following from the root directory of your checked-out source repository . Recommended RAM Given that our test setup creates the container images and then launches a test cluster, we recommend that you have at least 4GB of RAM available for the tests. With the prerequisites out of the way, let's move on to the first step: building the code generator.","title":"Prerequisites"},{"location":"dev-docs/testing/#build-code-generator","text":"To build the latest ack-generate binary, execute the following command: 1 make build-ack-generate One-off build You only have to do this once, overall. In other words: unless we change something upstream in terms of the code generation process, this is a one-off operation. Internally, the Makefile executes an go build here. Don't worry if you forget this step, the script in the next step will complain with a message along the line of ERROR: Unable to find an ack-generate binary and will give you another opportunity to rectify the situation.","title":"Build code generator"},{"location":"dev-docs/testing/#build-an-ack-service-controller","text":"Now that we have the basic code generation step done we will create the respective ACK service controller and its supporting artifacts. So first you have to select a service that you want to build and test. You do that by setting the SERVICE environment variable. Let's say we want to test the S3 service (creating an S3 bucket), so we would execute the following: 1 export SERVICE = s3 Now we are in a position to generate the ACK service controller for the S3 API. The following outputs the generated code to the services/$SERVICE directory: 1 make build-controller SERVICE=$SERVICE Handle controller-gen: command not found If you run into the controller-gen: command not found message when executing make build-controller then you want to check if the controller-gen binary is available in $GOPATH/bin , see also #234 . In addition to the ACK service controller code, above generates the custom resource definition (CRD) manifests as well as the necessary RBAC settings using the build-controller.sh script. Beyond dev preview In future, this step will also generate the end-user install artifacts, that is the Helm chart etc. that can be used to install those CRD manifests, and a deployment manifest that runs the ACK service controller in a pod. Now that we have the generation part completed, we want to see if the generated artifacts indeed are able to create an S3 bucket for us. You don't have to do the next steps, if all you want to test is if the generation works, however, for an end-to-end test, the next, final step is a necessary one.","title":"Build an ACK service controller"},{"location":"dev-docs/testing/#run-tests","text":"Time to run the end-to-end test.","title":"Run tests"},{"location":"dev-docs/testing/#iam-setup","text":"In order for the ACK service controller to manage the S3 bucket, it needs an identity. In other words, it needs an IAM role that represents the ACK service controller towards the S3 service. First, define the name of the IAM role that will have the permission to manage S3 buckets on your behalf: 1 export ACK_TEST_IAM_ROLE = Admin - k8s Now we need to verify the IAM principal (likely an IAM user) that is going to assume the IAM role ACK_TEST_IAM_ROLE . So to get its ARN, execute: 1 export ACK_TEST_PRINCIPAL_ARN =$ ( aws sts get - caller - identity -- query 'Arn' -- output text ) You can verify if that worked using echo $ACK_TEST_PRINCIPAL_ARN and that should print something along the lines of arn:aws:iam::1234567890121:user/ausername . Next up, create the IAM role, adding the necessary trust relationship to the role, using the following commands: 1 2 3 4 5 6 7 8 9 10 11 12 cat > trust-policy.json << EOF { \"Version\": \"2012-10-17\", \"Statement\": { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"$ACK_TEST_PRINCIPAL_ARN\" }, \"Action\": \"sts:AssumeRole\" } } EOF Using above trust policy, we can now create the IAM role: 1 2 3 aws iam create-role \\ --role-name $ACK_TEST_IAM_ROLE \\ --assume-role-policy-document file://trust-policy.json Now we're in the position to give the IAM role ACK_TEST_IAM_ROLE the permission to handle S3 buckets for us, using: 1 2 3 aws iam attach-role-policy \\ --role-name $ACK_TEST_IAM_ROLE \\ --policy-arn \"arn:aws:iam::aws:policy/AmazonS3FullAccess\" Access delegation in IAM If you're not that familiar with IAM access delegation, we recommend you to peruse the IAM documentation Next, in order for our test to generate temporary credentials we need to tell it to use the IAM role we created in the previous step. To generate the IAM role ARN, do: 1 2 AWS_ACCOUNT_ID =$ ( aws sts get - caller - identity -- query 'Account' -- output text ) && \\ export AWS_ROLE_ARN = arn : aws : iam :: $ { AWS_ACCOUNT_ID }: role /$ { ACK_TEST_IAM_ROLE } Info The tests uses the generate_temp_creds function from the scripts/lib/aws.sh script, executing effectively aws sts assume-role --role-session-arn $AWS_ROLE_ARN --role-session-name $TEMP_ROLE which fetches temporarily AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and an AWS_SESSION_TOKEN used in turn to authentication the ACK controller. The duration of the session token is 900 seconds (15 minutes). Phew that was a lot to set up, but good news: you're almost there.","title":"IAM setup"},{"location":"dev-docs/testing/#run-end-to-end-test","text":"Before you proceed, make sure that you've done the IAM setup in the previous step. IAM troubles?! If you try the following command and you see an error message containing something along the line of AWS_ROLE_ARN is not defined. then you know that somewhere in the IAM setup you either left out a step or one of the commands failed. Now we're finally in the position to execute the end-to-end test: 1 make kind-test SERVICE=$SERVICE This provisions a Kubernetes cluster using kind , builds a container image with the ACK service controller, and loads the container image into the kind cluster. It then installs the ACK service controller and related Kubernetes manifests into the kind cluster using kustomize build | kubectl apply -f - . Then, the above script runs a series of test scripts that call kubectl and the aws CLI tools to verify that custom resources of the type managed by the respective ACK service controller is created, updated and deleted appropriately (still TODO). Finally, it will run tests that create resources for the respective service and verify if the resource has successfully created. In our example case it should create an S3 bucket and then destroy it again, yielding something like the following (edited down to the relevant parts): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ... ./ scripts / kind - build - test . sh - s s3 Using Kubernetes kindest / node : v1 . 16.9 @ sha256 : 7175872357 bc85847ec4b1aba46ed1d12fa054c83ac7a8a11f5c268957fd5765 Creating k8s cluster using \"kind\" ... No kind clusters found . Created k8s cluster using \"kind\" Building s3 docker image Building 's3' controller docker image with tag : ack - s3 - controller : ec452ed sha256 : c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef Loading the images into the cluster Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-worker\" , loading ... Image : \"ack-s3-controller:ec452ed\" with ID \"sha256:c9cbcc028f2b7351d0507f8542ab88c80f9fb5a3b8b800feee8e362882833eef\" not yet present on node \"test-ccc3c7f1-control-plane\" , loading ... Loading CRD manifests for s3 into the cluster customresourcedefinition . apiextensions . k8s . io / buckets . s3 . services . k8s . aws created Loading RBAC manifests for s3 into the cluster clusterrole . rbac . authorization . k8s . io / ack - controller - role created clusterrolebinding . rbac . authorization . k8s . io / ack - controller - rolebinding created Loading service controller Deployment for s3 into the cluster 2020 / 08 / 18 09 : 51 : 46 Fixed the missing field by adding apiVersion : kustomize . config . k8s . io / v1beta1 Fixed the missing field by adding kind : Kustomization namespace / ack - system created deployment . apps / ack - s3 - controller created Running aws sts assume - role -- role - arn arn : aws : iam :: 1234567890121 : role / Admin - k8s , -- role - session - name tmp - role - 1 b779de5 -- duration - seconds 900 , Temporary credentials generated deployment . apps / ack - s3 - controller env updated Added AWS Credentials to env vars map ====================================================================================================== To poke around your test manually : export KUBECONFIG =/ Users / hausenbl / ACK / upstream / aws - controllers - k8s / scripts /../ build / tmp - test - ccc3c7f1 / kubeconfig kubectl get pods - A ====================================================================================================== bucket . s3 . services . k8s . aws / ack - test - smoke - s3 created { \"Name\" : \"ack-test-smoke-s3\" , \"CreationDate\" : \"2020-08-18T08:52:04+00:00\" } bucket . s3 . services . k8s . aws \"ack-test-smoke-s3\" deleted smoke took 27 second ( s ) \ud83e\udd51 Deleting k8s cluster using \"kind\" Deleting cluster \"test-ccc3c7f1\" ... As you can see, in above case the end-to-end test (creating cluster, deploying ACK, applying custom resources, and tear-down) took less than 30 seconds. This is for the warmed caches case.","title":"Run end-to-end test"},{"location":"dev-docs/testing/#background","text":"We use mockery for unit testing. You can install it by following the guideline on mockery's GitHub or simply by running our handy scirpt at ./scripts/install_mockery.sh for general Linux environments. We track testing in the umbrella issue 6 . on GitHub. Use this issue as a starting point and if you create a new testing-related issue, mention it from there.","title":"Background"},{"location":"dev-docs/testing/#clean-up","text":"To clean up a kind cluster, including the container images and configuration files created by the script specifically for said test cluster, execute: 1 kind delete cluster --name $CLUSTER_NAME If you want to delete all kind cluster running on your machine, use: 1 make delete-all-kind-clusters With this the testing is completed. Thanks for your time and we appreciate your feedback.","title":"Clean up"},{"location":"user-docs/authorization/","text":"Authorization \u00b6 When we talk about authorization and access control for ACK, we need to discuss two different Role-based Access Control (RBAC) systems. Remember that Kubernetes RBAC governs a Kubernetes user's ability to read or write Kubernetes resources . In the case of ACK, this means that Kubernetes RBAC system controls the ability of a Kubernetes user to read or write different custom resources (CRs) that ACK service controllers use. On the other end of the authorization spectrum, you can use AWS Identity and Access Management (IAM) Policies to governs the ability of an AWS IAM Role to read or write certain AWS resources . IAM is more than RBAC AWS IAM is more than just an RBAC system. It handles authentication/identification and can be used to build Attribute-based Access Control (ABAC) systems. In this document, however, we're focusing on using IAM primitives to establish an RBAC system. These two RBAC systems do not overlap . The Kubernetes user that calls the Kubernetes API via calls to kubectl has no association with an IAM Role . Instead, it is the Service Account running the ACK service controller's Pod that is associated with an IAM Role and is thus governed by the IAM RBAC system. RBAC authorization mode The above diagram assumes you are running Kubernetes API server with the RBAC authorization mode enabled. Configure permissions \u00b6 Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions. Configuring Kubernetes RBAC \u00b6 As part of installation, certain Kubernetes Role resources will be created that contain permissions to modify the Kubernetes custom resources (CRs) that the ACK service controller is responsible for. Important All Kubernetes CRs managed by an ACK service controller are Namespaced resources; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role resources are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resources that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resources that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRDs and CRs associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack-user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack-reader Role would have been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack-user Role to read/write CRs of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CRs of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role resources have been created, you will want to assign a specific Kubernetes User to a particular Role . You do this using standard Kubernetes RoleBinding resource. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack-user apiGroup : rbac.authorization.k8s.io As always, if you are curious whether a particular Kubernetes user can perform some action on a Kubernetes resource, you can use the kubectl auth can-i command, like this example shows: 1 kubectl auth can-i create buckets --namespace default Configuring AWS IAM \u00b6 Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. The IAM Role that your ACK service controller runs as will need a different set of IAM Policies depending on which AWS service API the service controller is managing. For instance, the ACK service controller for S3 will need permissions to read and write S3 Buckets. We include with each service controller a recommended IAM Policy that restricts the ACK service controller to taking only the actions that the IAM Role needs to properly manage resources for that specific AWS service API. Within each service controller's source code directory is a config/iam/recommended-policy-arn document that contains the AWS Resource Name (ARN) of the recommended managed policy for that service and can be applied to the IAM Role for the ACK service controller by calling aws iam attach-role-policy on the contents of that file: 1 2 3 4 5 6 7 SERVICE = s3 BASE_URL = https://github.com/aws/aws-controllers-k8s/blob/main/services POLICY_URL = $BASE_URL / $SERVICE /config/iam/recommended-policy-arn POLICY_ARN = \"`wget -qO- $POLICY_URL `\" aws iam attach-role-policy \\ --role-name $IAM_ROLE \\ --policy-arn $POLICY_ARN Note Set the $IAM_ROLE variable above to the ARN of the IAM Role the ACK service controller will run as. Cross-account resource management \u00b6 TODO","title":"Authorization"},{"location":"user-docs/authorization/#authorization","text":"When we talk about authorization and access control for ACK, we need to discuss two different Role-based Access Control (RBAC) systems. Remember that Kubernetes RBAC governs a Kubernetes user's ability to read or write Kubernetes resources . In the case of ACK, this means that Kubernetes RBAC system controls the ability of a Kubernetes user to read or write different custom resources (CRs) that ACK service controllers use. On the other end of the authorization spectrum, you can use AWS Identity and Access Management (IAM) Policies to governs the ability of an AWS IAM Role to read or write certain AWS resources . IAM is more than RBAC AWS IAM is more than just an RBAC system. It handles authentication/identification and can be used to build Attribute-based Access Control (ABAC) systems. In this document, however, we're focusing on using IAM primitives to establish an RBAC system. These two RBAC systems do not overlap . The Kubernetes user that calls the Kubernetes API via calls to kubectl has no association with an IAM Role . Instead, it is the Service Account running the ACK service controller's Pod that is associated with an IAM Role and is thus governed by the IAM RBAC system. RBAC authorization mode The above diagram assumes you are running Kubernetes API server with the RBAC authorization mode enabled.","title":"Authorization"},{"location":"user-docs/authorization/#configure-permissions","text":"Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions.","title":"Configure permissions"},{"location":"user-docs/authorization/#configuring-kubernetes-rbac","text":"As part of installation, certain Kubernetes Role resources will be created that contain permissions to modify the Kubernetes custom resources (CRs) that the ACK service controller is responsible for. Important All Kubernetes CRs managed by an ACK service controller are Namespaced resources; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role resources are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resources that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resources that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRDs and CRs associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack-user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack-reader Role would have been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack-user Role to read/write CRs of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CRs of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role resources have been created, you will want to assign a specific Kubernetes User to a particular Role . You do this using standard Kubernetes RoleBinding resource. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack-user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack-user apiGroup : rbac.authorization.k8s.io As always, if you are curious whether a particular Kubernetes user can perform some action on a Kubernetes resource, you can use the kubectl auth can-i command, like this example shows: 1 kubectl auth can-i create buckets --namespace default","title":"Configuring Kubernetes RBAC"},{"location":"user-docs/authorization/#configuring-aws-iam","text":"Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. The IAM Role that your ACK service controller runs as will need a different set of IAM Policies depending on which AWS service API the service controller is managing. For instance, the ACK service controller for S3 will need permissions to read and write S3 Buckets. We include with each service controller a recommended IAM Policy that restricts the ACK service controller to taking only the actions that the IAM Role needs to properly manage resources for that specific AWS service API. Within each service controller's source code directory is a config/iam/recommended-policy-arn document that contains the AWS Resource Name (ARN) of the recommended managed policy for that service and can be applied to the IAM Role for the ACK service controller by calling aws iam attach-role-policy on the contents of that file: 1 2 3 4 5 6 7 SERVICE = s3 BASE_URL = https://github.com/aws/aws-controllers-k8s/blob/main/services POLICY_URL = $BASE_URL / $SERVICE /config/iam/recommended-policy-arn POLICY_ARN = \"`wget -qO- $POLICY_URL `\" aws iam attach-role-policy \\ --role-name $IAM_ROLE \\ --policy-arn $POLICY_ARN Note Set the $IAM_ROLE variable above to the ARN of the IAM Role the ACK service controller will run as.","title":"Configuring AWS IAM"},{"location":"user-docs/authorization/#cross-account-resource-management","text":"TODO","title":"Cross-account resource management"},{"location":"user-docs/install/","text":"Install \u00b6 In the following we walk you through installing an AWS service controller. Helm (recommended) \u00b6 The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-controllers-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3 Static Kubernetes manifests \u00b6 If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACK service controllers, make sure to configure permissions , next.","title":"Install"},{"location":"user-docs/install/#install","text":"In the following we walk you through installing an AWS service controller.","title":"Install"},{"location":"user-docs/install/#helm-recommended","text":"The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-controllers-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3","title":"Helm (recommended)"},{"location":"user-docs/install/#static-kubernetes-manifests","text":"If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACK service controllers, make sure to configure permissions , next.","title":"Static Kubernetes manifests"},{"location":"user-docs/usage/","text":"Usage \u00b6 In this section we discuss how to use AWS Controllers for Kubernetes. Prerequisites \u00b6 Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles. Creating an AWS resource via the Kubernetes API \u00b6 TODO Viewing AWS resource information via the Kubernetes API \u00b6 TODO Deleting an AWS resource via the Kubernetes API \u00b6 TODO Modifying an AWS resource via the Kubernetes API \u00b6 TODO","title":"Usage"},{"location":"user-docs/usage/#usage","text":"In this section we discuss how to use AWS Controllers for Kubernetes.","title":"Usage"},{"location":"user-docs/usage/#prerequisites","text":"Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles.","title":"Prerequisites"},{"location":"user-docs/usage/#creating-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Creating an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#viewing-aws-resource-information-via-the-kubernetes-api","text":"TODO","title":"Viewing AWS resource information via the Kubernetes API"},{"location":"user-docs/usage/#deleting-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Deleting an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#modifying-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Modifying an AWS resource via the Kubernetes API"},{"location":"user-docs/wip/","text":"Work in progress ... \u00b6 ... check back later for end-user facing install and usage of ACK. Tip Use the developer preview and let us know via the issue tracker if something doesn't work as described there.","title":"Usage"},{"location":"user-docs/wip/#work-in-progress","text":"... check back later for end-user facing install and usage of ACK. Tip Use the developer preview and let us know via the issue tracker if something doesn't work as described there.","title":"Work in progress ..."}]}